{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Setup libraries and constants"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pathlib\n",
    "import time\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "from PIL import Image\n",
    "from tensorflow_addons.image import gaussian_filter2d\n",
    "\n",
    "task = 'flower' # flower = 25 mins/epoch\n",
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.8\n",
    "tf.compat.v1.keras.backend.set_session(tf.compat.v1.Session(config=config))\n",
    "\n",
    "base_path = \"../\"\n",
    "CHECKPOINT_PATH = base_path+\"res/checkpoints/train\"\n",
    "\n",
    "add_path = 'jpg/' if task == 'flower' else ''\n",
    "\n",
    "x_train = pathlib.Path(base_path+\"res/dataset/x/train/\"+add_path)\n",
    "y_train = pathlib.Path(base_path+\"res/dataset/y/train/\"+add_path)\n",
    "x_test = pathlib.Path(base_path+\"res/dataset/x/test/\"+add_path)\n",
    "y_test = pathlib.Path(base_path+\"res/dataset/y/test/\"+add_path)\n",
    "x_val = pathlib.Path(base_path+\"res/dataset/x/val/\"+add_path)\n",
    "y_val = pathlib.Path(base_path+\"es/dataset/y/val/\"+add_path)\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "RESIZE_H = 286\n",
    "RESIZE_W = 286\n",
    "\n",
    "BUFFER_SIZE = 500\n",
    "BATCH_SIZE = 1\n",
    "IMG_WIDTH = 256\n",
    "IMG_HEIGHT = 256\n",
    "\n",
    "LR = 2e-4\n",
    "NORM = 'instancenorm'  # 'batchnorm'\n",
    "LAMBDA = 10\n",
    "BETA = 0.5  # 0.999\n",
    "DISC_TERM = 0.5\n",
    "ID_TERM = 0.5\n",
    "\n",
    "EPOCHS = 100\n",
    "EPOCH_CHECKPOINT = 1\n",
    "MAX_CHECKPOINT = 5\n",
    "\n",
    "sigma_blur = 0 if task == 'flower' else 1\n",
    "filter_shape = (4, 4)\n",
    "\n",
    "OUTPUT_CHANNELS = 1 if task == 'flower' else 3\n",
    "OUTPUT_CHANNELS_G = 3\n",
    "OUTPUT_CHANNELS_F = OUTPUT_CHANNELS\n",
    "\n",
    "loss_obj = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Prepare the dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "list_ds_x_train = tf.data.Dataset.list_files(str(x_train/'*'))\n",
    "list_ds_y_train = tf.data.Dataset.list_files(str(y_train/'*'))\n",
    "\n",
    "#for f in list_ds_x_train.take(1):\n",
    "#    print(f.numpy())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def decode_img_bw(img):\n",
    "    # convert the compressed string to a 3D uint8 tensor\n",
    "    img = tf.image.decode_jpeg(img, channels=OUTPUT_CHANNELS)\n",
    "    # Use `convert_image_dtype` to convert to floats in the [0,1] range.\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    # resize the image to the desired size.\n",
    "    return tf.image.resize(img, [IMG_WIDTH, IMG_HEIGHT])\n",
    "    #return img\n",
    "\n",
    "def decode_img_rgb(img):\n",
    "    # convert the compressed string to a 3D uint8 tensor\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    # Use `convert_image_dtype` to convert to floats in the [0,1] range.\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    # resize the image to the desired size.\n",
    "    return tf.image.resize(img, [IMG_WIDTH, IMG_HEIGHT])\n",
    "    #return img\n",
    "\n",
    "def process_path_bw(file_path):\n",
    "    # load the raw data from the file as a string\n",
    "    img = tf.io.read_file(file_path)\n",
    "    img = decode_img_bw(img)\n",
    "    return img\n",
    "\n",
    "def process_path_rgb(file_path):\n",
    "    # load the raw data from the file as a string\n",
    "    img = tf.io.read_file(file_path)\n",
    "    img = decode_img_rgb(img)\n",
    "    return img\n",
    "\n",
    "ds_x_train = list_ds_x_train.map(process_path_bw, num_parallel_calls=AUTOTUNE)\n",
    "ds_y_train = list_ds_y_train.map(process_path_rgb, num_parallel_calls=AUTOTUNE)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "\"\\nimg_x = sample_x.numpy()[0]*255 # and comment normalize\\ncv2.imwrite('/home/alby/Downloads/image_x.jpg', img_x)\\nimg_y = sample_y.numpy()[0]*255 # and comment normalize\\ncv2.imwrite('/home/alby/Downloads/image_y.jpg', cv2.cvtColor(img_y, cv2.COLOR_RGB2BGR))\\n\""
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def random_crop(image):\n",
    "    cropped_image = tf.image.random_crop(image, size=[IMG_HEIGHT, IMG_WIDTH, OUTPUT_CHANNELS])\n",
    "\n",
    "    return cropped_image\n",
    "\n",
    "# normalizing the images to [-1, 1]\n",
    "def normalize(image):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = (image / 127.5) - 1\n",
    "    return image\n",
    "\n",
    "def random_jitter(image):\n",
    "    # resizing to 286 x 286 x 3\n",
    "    image = tf.image.resize(image, [RESIZE_H, RESIZE_W],\n",
    "                          method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "\n",
    "    # randomly cropping to 256 x 256 x 3\n",
    "    image = random_crop(image)\n",
    "\n",
    "    # random mirroring\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "\n",
    "    return image\n",
    "\n",
    "def blur(image):\n",
    "    res = gaussian_filter2d(image, filter_shape=filter_shape, sigma=sigma_blur)\n",
    "    return res\n",
    "\n",
    "def preprocess_image_train_x(image):\n",
    "    if task != 'flower':\n",
    "        image = random_jitter(image)\n",
    "    image = normalize(image)\n",
    "    return image\n",
    "\n",
    "def preprocess_image_train_y(image):\n",
    "    if task != 'flower':\n",
    "        image = random_jitter(image)\n",
    "    if sigma_blur > 0:\n",
    "        image = blur(image)\n",
    "    image = normalize(image)\n",
    "    return image\n",
    "\n",
    "def preprocess_image_test(image):\n",
    "    image = normalize(image)\n",
    "    return image\n",
    "\n",
    "ds_x_train = ds_x_train.map(\n",
    "    preprocess_image_train_x, num_parallel_calls=AUTOTUNE).cache().shuffle(\n",
    "    BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "\n",
    "ds_y_train = ds_y_train.map(\n",
    "    preprocess_image_train_y, num_parallel_calls=AUTOTUNE).cache().shuffle(\n",
    "    BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "\n",
    "\"\"\"test_horses = test_horses.map(\n",
    "    preprocess_image_test, num_parallel_calls=AUTOTUNE).cache().shuffle(\n",
    "    BUFFER_SIZE).batch(1)\"\"\"\n",
    "\n",
    "sample_x = next(iter(ds_x_train))\n",
    "sample_y = next(iter(ds_y_train))\n",
    "\"\"\"\n",
    "img_x = sample_x.numpy()[0]*255 # and comment normalize\n",
    "cv2.imwrite('/home/alby/Downloads/image_x.jpg', img_x)\n",
    "img_y = sample_y.numpy()[0]*255 # and comment normalize\n",
    "cv2.imwrite('/home/alby/Downloads/image_y.jpg', cv2.cvtColor(img_y, cv2.COLOR_RGB2BGR))\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Generator and discriminator"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "class InstanceNormalization(tf.keras.layers.Layer):\n",
    "    \"\"\"Instance Normalization Layer (https://arxiv.org/abs/1607.08022).\"\"\"\n",
    "    def __init__(self, epsilon=1e-5):\n",
    "        super(InstanceNormalization, self).__init__()\n",
    "        self.epsilon = epsilon\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.scale = self.add_weight(\n",
    "            name='scale',\n",
    "            shape=input_shape[-1:],\n",
    "            initializer=tf.random_normal_initializer(1., 0.02),\n",
    "            trainable=True)\n",
    "        \n",
    "        self.offset = self.add_weight(\n",
    "            name='offset',\n",
    "            shape=input_shape[-1:],\n",
    "            initializer='zeros',\n",
    "            trainable=True)\n",
    "    \n",
    "    def call(self, x):\n",
    "        mean, variance = tf.nn.moments(x, axes=[1, 2], keepdims=True)\n",
    "        inv = tf.math.rsqrt(variance + self.epsilon)\n",
    "        normalized = (x - mean) * inv\n",
    "        return self.scale * normalized + self.offset\n",
    "  \n",
    "def downsample(filters, size, norm_type='batchnorm', apply_norm=True):\n",
    "    \"\"\"Downsamples an input.\n",
    "    Conv2D => Batchnorm => LeakyRelu\n",
    "    Args:\n",
    "    filters: number of filters\n",
    "    size: filter size\n",
    "    norm_type: Normalization type; either 'batchnorm' or 'instancenorm'.\n",
    "    apply_norm: If True, adds the batchnorm layer\n",
    "    Returns:\n",
    "    Downsample Sequential Model\n",
    "    \"\"\"\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "    \n",
    "    result = tf.keras.Sequential()\n",
    "    result.add(\n",
    "      tf.keras.layers.Conv2D(filters, size, strides=2, padding='same',\n",
    "                             kernel_initializer=initializer, use_bias=False))\n",
    "    \n",
    "    if apply_norm:\n",
    "        if norm_type.lower() == 'batchnorm':\n",
    "            result.add(tf.keras.layers.BatchNormalization())\n",
    "    elif norm_type.lower() == 'instancenorm':\n",
    "        result.add(InstanceNormalization())\n",
    "    \n",
    "    result.add(tf.keras.layers.LeakyReLU())\n",
    "    \n",
    "    return result\n",
    "\n",
    "def upsample(filters, size, norm_type='batchnorm', apply_dropout=False):\n",
    "    \"\"\"Upsamples an input.\n",
    "    Conv2DTranspose => Batchnorm => Dropout => Relu\n",
    "    Args:\n",
    "    filters: number of filters\n",
    "    size: filter size\n",
    "    norm_type: Normalization type; either 'batchnorm' or 'instancenorm'.\n",
    "    apply_dropout: If True, adds the dropout layer\n",
    "    Returns:\n",
    "    Upsample Sequential Model\n",
    "    \"\"\"\n",
    "    \n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "    \n",
    "    result = tf.keras.Sequential()\n",
    "    result.add(\n",
    "      tf.keras.layers.Conv2DTranspose(filters, size, strides=2,\n",
    "                                      padding='same',\n",
    "                                      kernel_initializer=initializer,\n",
    "                                      use_bias=False))\n",
    "    \n",
    "    if norm_type.lower() == 'batchnorm':\n",
    "        result.add(tf.keras.layers.BatchNormalization())\n",
    "    elif norm_type.lower() == 'instancenorm':\n",
    "        result.add(InstanceNormalization())\n",
    "    \n",
    "    if apply_dropout:\n",
    "        result.add(tf.keras.layers.Dropout(0.5))\n",
    "    \n",
    "    result.add(tf.keras.layers.ReLU())\n",
    "    \n",
    "    return result\n",
    "\n",
    "def unet_generator(input_channels, output_channels, norm_type='batchnorm'):\n",
    "    \"\"\"Modified u-net generator model (https://arxiv.org/abs/1611.07004).\n",
    "    Args:\n",
    "    output_channels: Output channels\n",
    "    norm_type: Type of normalization. Either 'batchnorm' or 'instancenorm'.\n",
    "    Returns:\n",
    "    Generator model\n",
    "    \"\"\"\n",
    "    \n",
    "    down_stack = [\n",
    "        downsample(64, 4, norm_type, apply_norm=False),  # (bs, 128, 128, 64)\n",
    "        downsample(128, 4, norm_type),  # (bs, 64, 64, 128)\n",
    "        downsample(256, 4, norm_type),  # (bs, 32, 32, 256)\n",
    "        downsample(512, 4, norm_type),  # (bs, 16, 16, 512)\n",
    "        downsample(512, 4, norm_type),  # (bs, 8, 8, 512)\n",
    "        downsample(512, 4, norm_type),  # (bs, 4, 4, 512)\n",
    "        downsample(512, 4, norm_type),  # (bs, 2, 2, 512)\n",
    "        downsample(512, 4, norm_type),  # (bs, 1, 1, 512)\n",
    "    ]\n",
    "    \n",
    "    up_stack = [\n",
    "        upsample(512, 4, norm_type, apply_dropout=True),  # (bs, 2, 2, 1024)\n",
    "        upsample(512, 4, norm_type, apply_dropout=True),  # (bs, 4, 4, 1024)\n",
    "        upsample(512, 4, norm_type, apply_dropout=True),  # (bs, 8, 8, 1024)\n",
    "        upsample(512, 4, norm_type),  # (bs, 16, 16, 1024)\n",
    "        upsample(256, 4, norm_type),  # (bs, 32, 32, 512)\n",
    "        upsample(128, 4, norm_type),  # (bs, 64, 64, 256)\n",
    "        upsample(64, 4, norm_type),  # (bs, 128, 128, 128)\n",
    "    ]\n",
    "    \n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "    last = tf.keras.layers.Conv2DTranspose(\n",
    "      output_channels, 4, strides=2,\n",
    "      padding='same', kernel_initializer=initializer,\n",
    "      activation='tanh')  # (bs, 256, 256, 3)\n",
    "    \n",
    "    concat = tf.keras.layers.Concatenate()\n",
    "    \n",
    "    inputs = tf.keras.layers.Input(shape=[None, None, input_channels])\n",
    "    x = inputs\n",
    "    \n",
    "    # Downsampling through the model\n",
    "    skips = []\n",
    "    for down in down_stack:\n",
    "        x = down(x)\n",
    "        skips.append(x)\n",
    "        \n",
    "    skips = reversed(skips[:-1])\n",
    "    \n",
    "    # Upsampling and establishing the skip connections\n",
    "    for up, skip in zip(up_stack, skips):\n",
    "        x = up(x)\n",
    "        x = concat([x, skip])\n",
    "    \n",
    "    x = last(x)\n",
    "    \n",
    "    return tf.keras.Model(inputs=inputs, outputs=x)\n",
    "\n",
    "def discriminator(input_channels, norm_type='batchnorm', target=True):\n",
    "    \"\"\"PatchGan discriminator model (https://arxiv.org/abs/1611.07004).\n",
    "    Args:\n",
    "    norm_type: Type of normalization. Either 'batchnorm' or 'instancenorm'.\n",
    "    target: Bool, indicating whether target image is an input or not.\n",
    "    Returns:\n",
    "    Discriminator model\n",
    "    \"\"\"\n",
    "    \n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "    \n",
    "    inp = tf.keras.layers.Input(shape=[None, None, input_channels], name='input_image')\n",
    "    x = inp\n",
    "    \n",
    "    if target:  # TODO\n",
    "        tar = tf.keras.layers.Input(shape=[None, None, 3], name='target_image')\n",
    "        x = tf.keras.layers.concatenate([inp, tar])  # (bs, 256, 256, channels*2)\n",
    "    \n",
    "    down1 = downsample(64, 4, norm_type, False)(x)  # (bs, 128, 128, 64)\n",
    "    down2 = downsample(128, 4, norm_type)(down1)  # (bs, 64, 64, 128)\n",
    "    down3 = downsample(256, 4, norm_type)(down2)  # (bs, 32, 32, 256)\n",
    "    \n",
    "    zero_pad1 = tf.keras.layers.ZeroPadding2D()(down3)  # (bs, 34, 34, 256)\n",
    "    conv = tf.keras.layers.Conv2D(\n",
    "      512, 4, strides=1, kernel_initializer=initializer,\n",
    "      use_bias=False)(zero_pad1)  # (bs, 31, 31, 512)\n",
    "    \n",
    "    if norm_type.lower() == 'batchnorm':\n",
    "        norm1 = tf.keras.layers.BatchNormalization()(conv)\n",
    "    elif norm_type.lower() == 'instancenorm':\n",
    "        norm1 = InstanceNormalization()(conv)\n",
    "    \n",
    "    leaky_relu = tf.keras.layers.LeakyReLU()(norm1)\n",
    "    \n",
    "    zero_pad2 = tf.keras.layers.ZeroPadding2D()(leaky_relu)  # (bs, 33, 33, 512)\n",
    "    \n",
    "    last = tf.keras.layers.Conv2D(\n",
    "      1, 4, strides=1,\n",
    "      kernel_initializer=initializer)(zero_pad2)  # (bs, 30, 30, 1)\n",
    "    \n",
    "    if target:\n",
    "        return tf.keras.Model(inputs=[inp, tar], outputs=last)\n",
    "    else:\n",
    "        return tf.keras.Model(inputs=inp, outputs=last)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "generator_g = unet_generator(input_channels=OUTPUT_CHANNELS, output_channels=OUTPUT_CHANNELS_G, norm_type=NORM)\n",
    "generator_f = unet_generator(input_channels=3, output_channels=OUTPUT_CHANNELS_F, norm_type=NORM)\n",
    "\n",
    "discriminator_x = discriminator(input_channels=3, norm_type=NORM, target=False)\n",
    "discriminator_y = discriminator(input_channels=OUTPUT_CHANNELS, norm_type=NORM, target=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def discriminator_loss(real, generated):\n",
    "    real_loss = loss_obj(tf.ones_like(real), real)\n",
    "    \n",
    "    generated_loss = loss_obj(tf.zeros_like(generated), generated)\n",
    "    \n",
    "    total_disc_loss = real_loss + generated_loss\n",
    "    \n",
    "    return total_disc_loss * DISC_TERM\n",
    "\n",
    "def generator_loss(generated):\n",
    "    return loss_obj(tf.ones_like(generated), generated)\n",
    "\n",
    "def calc_cycle_loss(real_image, cycled_image):\n",
    "    loss1 = tf.reduce_mean(tf.abs(real_image - cycled_image))\n",
    "    \n",
    "    return LAMBDA * loss1\n",
    "\n",
    "def identity_loss(real_image, same_image):\n",
    "    loss = tf.reduce_mean(tf.abs(real_image - same_image))\n",
    "    return LAMBDA * ID_TERM * loss\n",
    "\n",
    "generator_g_optimizer = tf.keras.optimizers.Adam(LR, beta_1=BETA)\n",
    "generator_f_optimizer = tf.keras.optimizers.Adam(LR, beta_1=BETA)\n",
    "\n",
    "discriminator_x_optimizer = tf.keras.optimizers.Adam(LR, beta_1=BETA)\n",
    "discriminator_y_optimizer = tf.keras.optimizers.Adam(LR, beta_1=BETA)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Checkpoint"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "pathlib.Path(CHECKPOINT_PATH).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "ckpt = tf.train.Checkpoint(generator_g=generator_g,\n",
    "                           generator_f=generator_f,\n",
    "                           discriminator_x=discriminator_x,\n",
    "                           discriminator_y=discriminator_y,\n",
    "                           generator_g_optimizer=generator_g_optimizer,\n",
    "                           generator_f_optimizer=generator_f_optimizer,\n",
    "                           discriminator_x_optimizer=discriminator_x_optimizer,\n",
    "                           discriminator_y_optimizer=discriminator_y_optimizer)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, CHECKPOINT_PATH, max_to_keep=MAX_CHECKPOINT)\n",
    "\n",
    "if ckpt_manager.checkpoints:\n",
    "    epoch = ckpt_manager.checkpoints[-1]\n",
    "    loaded_epoch = int(''.join(c for c in epoch if c.isdigit()))\n",
    "else:\n",
    "    loaded_epoch = 0\n",
    "\n",
    "# if a checkpoint exists, restore the latest checkpoint\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    obj = ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    print('Latest checkpoint restored')\n",
    "  \n",
    "def generate_images(model, test_input):\n",
    "    prediction = model(test_input)\n",
    "\n",
    "    test_input = cv2.normalize(test_input.numpy(), None, alpha = 0, beta = 255, norm_type = cv2.NORM_MINMAX,\n",
    "                               dtype = cv2.CV_32F).astype(np.uint8)\n",
    "    prediction = cv2.normalize(prediction.numpy(), None, alpha = 0, beta = 255, norm_type = cv2.NORM_MINMAX,\n",
    "                               dtype = cv2.CV_32F).astype(np.uint8)\n",
    "\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    \n",
    "    display_list = [test_input[0], prediction[0]]\n",
    "    title = ['Input Image', 'Predicted Image']\n",
    "    \n",
    "    for i in range(2):\n",
    "        plt.subplot(1, 2, i+1)\n",
    "        plt.title(title[i])\n",
    "        plt.imshow(display_list[i])\n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(real_x, real_y):\n",
    "    # persistent is set to True because the tape is used more than\n",
    "    # once to calculate the gradients.\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        # Generator G translates X -> Y\n",
    "        # Generator F translates Y -> X.\n",
    "        \n",
    "        fake_y = generator_g(real_x, training=True) # 3\n",
    "        cycled_x = generator_f(fake_y, training=True) # 1\n",
    "        \n",
    "        fake_x = generator_f(real_y, training=True) # 1\n",
    "        cycled_y = generator_g(fake_x, training=True) # 3\n",
    "\n",
    "        '''\n",
    "        g_g: in=1, out=3\n",
    "        g_f: in=3, out=1\n",
    "        d_x: in=3, out=1\n",
    "        d_y: in=1, out=1\n",
    "\n",
    "        real_x = 1\n",
    "        real_y = 3\n",
    "\n",
    "        fake_x = 1\n",
    "        fake_y = 3\n",
    "\n",
    "        cycled_x = 1\n",
    "        cycled_y = 3\n",
    "        '''\n",
    "        if task == 'flower':\n",
    "            # g_f wants as input a rgb but image is grayscale\n",
    "            #   real_x = grayscale_to_rgb(real_x), where real_x = grayscale\n",
    "            # g_g wants as input a grayscale but image is rgb\n",
    "            #   real_y = rgb_to_grayscale(real_y), where real_y = rgb\n",
    "            real_x_app = tf.image.grayscale_to_rgb(real_x)\n",
    "            real_y_app = tf.image.rgb_to_grayscale(real_y)\n",
    "            fake_x_app = tf.image.grayscale_to_rgb(fake_x)\n",
    "            fake_y_app = tf.image.rgb_to_grayscale(fake_y)\n",
    "        else:\n",
    "            real_x_app = real_x\n",
    "            real_y_app = real_y\n",
    "            fake_x_app = fake_x\n",
    "            fake_y_app = fake_y\n",
    "\n",
    "        # same_x and same_y are used for identity loss.\n",
    "        same_x = generator_f(real_x_app, training=True)\n",
    "        same_y = generator_g(real_y_app, training=True)\n",
    "        \n",
    "        disc_real_x = discriminator_x(real_x_app, training=True)\n",
    "        disc_real_y = discriminator_y(real_y_app, training=True)\n",
    "        \n",
    "        disc_fake_x = discriminator_x(fake_x_app, training=True)\n",
    "        disc_fake_y = discriminator_y(fake_y_app, training=True)\n",
    "        \n",
    "        # calculate the loss\n",
    "        gen_g_loss = generator_loss(disc_fake_y)\n",
    "        gen_f_loss = generator_loss(disc_fake_x)\n",
    "        \n",
    "        total_cycle_loss = calc_cycle_loss(real_x, cycled_x) + calc_cycle_loss(real_y, cycled_y)\n",
    "        \n",
    "        # Total generator loss = adversarial loss + cycle loss\n",
    "        total_gen_g_loss = gen_g_loss + total_cycle_loss + identity_loss(real_y, same_y)\n",
    "        total_gen_f_loss = gen_f_loss + total_cycle_loss + identity_loss(real_x, same_x)\n",
    "        \n",
    "        disc_x_loss = discriminator_loss(disc_real_x, disc_fake_x)\n",
    "        disc_y_loss = discriminator_loss(disc_real_y, disc_fake_y)\n",
    "  \n",
    "    # Calculate the gradients for generator and discriminator\n",
    "    generator_g_gradients = tape.gradient(total_gen_g_loss, \n",
    "                                        generator_g.trainable_variables)\n",
    "    generator_f_gradients = tape.gradient(total_gen_f_loss, \n",
    "                                        generator_f.trainable_variables)\n",
    "    \n",
    "    discriminator_x_gradients = tape.gradient(disc_x_loss, \n",
    "                                            discriminator_x.trainable_variables)\n",
    "    discriminator_y_gradients = tape.gradient(disc_y_loss, \n",
    "                                            discriminator_y.trainable_variables)\n",
    "    \n",
    "    # Apply the gradients to the optimizer\n",
    "    generator_g_optimizer.apply_gradients(zip(generator_g_gradients, \n",
    "                                            generator_g.trainable_variables))\n",
    "    \n",
    "    generator_f_optimizer.apply_gradients(zip(generator_f_gradients, \n",
    "                                            generator_f.trainable_variables))\n",
    "    \n",
    "    discriminator_x_optimizer.apply_gradients(zip(discriminator_x_gradients,\n",
    "                                                discriminator_x.trainable_variables))\n",
    "    \n",
    "    discriminator_y_optimizer.apply_gradients(zip(discriminator_y_gradients,\n",
    "                                                discriminator_y.trainable_variables))\n",
    "    return total_gen_g_loss, total_gen_f_loss, disc_x_loss, disc_y_loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2646 for epoch 0: 0.24 s. g_g: 2.086, g_f: 0.724, d_x: 0.694, d_y: 0.130"
     ]
    }
   ],
   "source": [
    "pathlib.Path('../res/logs/').mkdir(parents=True, exist_ok=True)\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "train_log_dir = '../res/logs/' + current_time + '/train'\n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "\n",
    "for epoch in range(loaded_epoch, EPOCHS):\n",
    "    start_epoch = time.time()\n",
    "    \n",
    "    n = 0\n",
    "    for image_x, image_y in tf.data.Dataset.zip((ds_x_train, ds_y_train)):\n",
    "        start_step = time.time()\n",
    "        g_g_loss, g_f_loss, d_x_loss, d_y_loss = train_step(image_x, image_y)\n",
    "        end_step = time.time()\n",
    "        with train_summary_writer.as_default():\n",
    "            tf.summary.scalar('loss_g_g', g_g_loss, step=epoch)\n",
    "            tf.summary.scalar('loss_g_f', g_f_loss, step=epoch)\n",
    "            tf.summary.scalar('loss_d_x', d_x_loss, step=epoch)\n",
    "            tf.summary.scalar('loss_d_y', d_y_loss, step=epoch)\n",
    "\n",
    "        n += 1\n",
    "        print(\"\\rStep {} for epoch {}: {:.2f} s. g_g: {:.3f}, g_f: {:.3f}, d_x: {:.3f}, d_y: {:.3f}\".format(n,\n",
    "            epoch, end_step - start_step, g_g_loss, g_f_loss, d_x_loss, d_y_loss), end='')\n",
    "\n",
    "    g_g_loss = 0\n",
    "    g_f_loss = 0\n",
    "    d_x_loss = 0\n",
    "    d_y_loss = 0\n",
    "\n",
    "    clear_output(wait=True)\n",
    "    # Using a consistent image (sample_horse) so that the progress of the model\n",
    "    # is clearly visible\n",
    "    generate_images(generator_g, sample_x)\n",
    "\n",
    "    if (epoch + 1) % EPOCH_CHECKPOINT == 0:\n",
    "        ckpt_save_path = ckpt_manager.save()\n",
    "        print('Saving checkpoint for epoch {} at {}'.format(epoch + 1, ckpt_save_path))\n",
    "    \n",
    "    end_epoch = time.time()\n",
    "    print('Time taken for epoch {} is {:.2f} s'.format(epoch + 1, end_epoch - start_epoch))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}